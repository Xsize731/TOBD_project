# ./airflow/dags/central_sales_simple.py
from datetime import datetime
import pandas as pd
from sqlalchemy import create_engine
from airflow import DAG
from airflow.operators.python import PythonOperator

def etl_central_region():
    logger = print  # –∏–ª–∏ –∏—Å–ø–æ–ª—å–∑—É–π—Ç–µ –ª–æ–≥–≥–µ—Ä, –∫–∞–∫ —Ä–∞–Ω—å—à–µ
    logger("üì• –ß—Ç–µ–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö...")
    df = pd.read_csv("/opt/airflow/data/raw/sales_data.csv", parse_dates=["transaction_date"])
    
    logger(f"–ü—Ä–æ—á–∏—Ç–∞–Ω–æ: {len(df)} —Å—Ç—Ä–æ–∫")

    logger("üîç –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è —Ä–µ–≥–∏–æ–Ω–∞ 'Central'...")
    df_central = df[df["region"] == "Central"]
    
    logger(f"–û—Ç–æ–±—Ä–∞–Ω–æ: {len(df_central)} —Å—Ç—Ä–æ–∫")

    logger("üì§ –ó–∞–ø–∏—Å—å –≤ sales_db.central_sales...")
    engine = create_engine("postgresql://airflow:airflow@postgres:5432/sales_db")
    df_central.to_sql("central_sales", engine, if_exists="replace", index=False)
    
    logger("‚úÖ –ì–æ—Ç–æ–≤–æ!")

with DAG(
    "central_sales_simple",
    start_date=datetime(2025, 12, 18),
    schedule_interval=None,
    catchup=False,
    tags=["etl", "sales"],
) as dag:

    PythonOperator(
        task_id="etl_central",
        python_callable=etl_central_region,
    )